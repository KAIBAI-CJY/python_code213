import os
os.environ["LOKY_MAX_CPU_COUNT"] = "4"  # CPU çº¿ç¨‹æ•°ï¼Œå¦‚ 4ã€8ã€16

import numpy as np
import pandas as pd
from sklearn.model_selection import KFold, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.linear_model import Ridge
from sklearn.metrics.pairwise import rbf_kernel
from deap import base, creator, tools, algorithms
from metrics import calculate_metrics
import random

# 1. è¯»å–æ•°æ®
file_path = r''
data = pd.read_excel(file_path, sheet_name='Sheet2')

X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values.reshape(-1, 1)

# 2. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

print(f"ğŸ” è®­ç»ƒé›†å¤§å°: {X_train.shape}, æµ‹è¯•é›†å¤§å°: {X_test.shape}")

# 3. å®šä¹‰ K æŠ˜äº¤å‰éªŒè¯
n_splits = 3
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# 4. å¯¹æ•°æ®æ ‡å‡†åŒ–
x_scaler = StandardScaler()
X_train_scaled = x_scaler.fit_transform(X_train)
X_test_scaled = x_scaler.transform(X_test)

y_scaler = StandardScaler()
y_train_scaled = y_scaler.fit_transform(y_train)
y_test_scaled = y_scaler.transform(y_test)

# 5. å®šä¹‰RBFNæ¨¡å‹ç±»
class SimpleRBFN:
    def __init__(self, n_centers=10, gamma=1.0, alpha=0.1):
        self.n_centers = n_centers
        self.gamma = gamma
        self.alpha = alpha
        self.kmeans = None
        self.model = None

    def fit(self, X, y):
        # ä½¿ç”¨K-meansç¡®å®šä¸­å¿ƒç‚¹
        self.kmeans = KMeans(n_clusters=self.n_centers, random_state=42)
        self.kmeans.fit(X)
        centers = self.kmeans.cluster_centers_
        
        # è®¡ç®—RBFç‰¹å¾
        rbf_features = rbf_kernel(X, centers, gamma=self.gamma)
        
        # ä½¿ç”¨å²­å›å½’é˜²æ­¢è¿‡æ‹Ÿåˆ
        self.model = Ridge(alpha=self.alpha)
        self.model.fit(rbf_features, y)
        
    def predict(self, X):
        centers = self.kmeans.cluster_centers_
        rbf_features = rbf_kernel(X, centers, gamma=self.gamma)
        return self.model.predict(rbf_features)

# 6. å®šä¹‰é—ä¼ ç®—æ³•ä¼˜åŒ–
creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
creator.create("Individual", list, fitness=creator.FitnessMin)

def create_individual():
    """ç”ŸæˆRBFNè¶…å‚æ•°ç»„åˆ"""
    n_centers = np.random.randint(5, 50)       # ä¸­å¿ƒç‚¹æ•°é‡
    gamma = 10**np.random.uniform(-3, 1)      # é«˜æ–¯æ ¸å®½åº¦: 0.001~10
    alpha = 10**np.random.uniform(-4, 0)      # æ­£åˆ™åŒ–ç³»æ•°: 0.0001~1 (ç¡®ä¿æ­£æ•°)
    return [n_centers, gamma, alpha]

def validate_params(individual):
    """å‚æ•°æœ‰æ•ˆæ€§éªŒè¯"""
    individual[0] = max(5, int(round(individual[0])))  # ç¡®ä¿ä¸­å¿ƒç‚¹æ•°â‰¥5
    individual[1] = max(1e-3, abs(individual[1]))  # å¼ºåˆ¶ gamma éè´Ÿ
    individual[2] = 10**np.clip(np.log10(abs(individual[2])), -5, 0)  # ç¡®ä¿ alpha ä¸º 10 çš„å¹‚æ¬¡
    return individual


def evaluate(individual):
    """è¯„ä¼°å‡½æ•°"""
    individual = validate_params(individual.copy())
    n_centers, gamma, alpha = individual
    
    fold_rmse = []
    for train_idx, valid_idx in kf.split(X_train_scaled):
        X_train_fold = X_train_scaled[train_idx]
        X_valid_fold = X_train_scaled[valid_idx]
        y_train_fold = y_train_scaled[train_idx]
        y_valid_fold = y_train_scaled[valid_idx]

        try:
            model = SimpleRBFN(n_centers=int(n_centers), 
                              gamma=gamma, 
                              alpha=alpha)
            model.fit(X_train_fold, y_train_fold)
            preds = model.predict(X_valid_fold)
            rmse = np.sqrt(np.mean((preds - y_valid_fold)**2))
            fold_rmse.append(rmse)
        except:
            fold_rmse.append(1e6)  # æ— æ•ˆå‚æ•°æƒ©ç½šå€¼

    return np.mean(fold_rmse),

# é—ä¼ ç®—æ³•é…ç½®
toolbox = base.Toolbox()
# å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡è¿è¡Œä¸€è‡´
random.seed(42)
np.random.seed(42)
toolbox.register("individual", tools.initIterate, creator.Individual, create_individual)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)
toolbox.register("mate", tools.cxBlend, alpha=0.5)  # æ··åˆäº¤å‰
toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.3)
toolbox.register("select", tools.selTournament, tournsize=3)
toolbox.register("evaluate", evaluate)

# 7. è¿è¡Œé—ä¼ ç®—æ³•ä¼˜åŒ–
population = toolbox.population(n=30)
generations = 15
cx_prob = 0.7
mut_prob = 0.3

for gen in range(generations):
    print(f"\n===== Generation {gen+1}/{generations} =====")
    fitnesses = toolbox.map(toolbox.evaluate, population)
    
    for ind, fit in zip(population, fitnesses):
        ind.fitness.values = fit

    offspring = toolbox.select(population, len(population))
    offspring = list(map(toolbox.clone, offspring))

    # äº¤å‰å’Œå˜å¼‚
    for child1, child2 in zip(offspring[::2], offspring[1::2]):
        if random.random() < cx_prob:
            toolbox.mate(child1, child2)
            del child1.fitness.values
            del child2.fitness.values

    for mutant in offspring:
        if random.random() < mut_prob:
            toolbox.mutate(mutant)
            del mutant.fitness.values

    # è¯„ä¼°æ–°ä¸ªä½“
    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)
    for ind, fit in zip(invalid_ind, fitnesses):
        ind.fitness.values = fit

    population[:] = offspring

# 8. æœ€ä¼˜æ¨¡å‹è®­ç»ƒ
best_individual = tools.selBest(population, 1)[0]
best_params = {
    'n_centers': int(best_individual[0]),
    'gamma': best_individual[1],
    'alpha': best_individual[2]
}
print(f"\n æœ€ä¼˜å‚æ•°: {best_params}")

final_model = SimpleRBFN(n_centers=best_params['n_centers'],
                       gamma=best_params['gamma'],
                       alpha=best_params['alpha'])
final_model.fit(X_train_scaled, y_train_scaled)

# 9. ç»“æœè¯„ä¼°ä¸ä¿å­˜
# å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
train_preds_scaled = final_model.predict(X_train_scaled)
test_preds_scaled = final_model.predict(X_test_scaled)

# åæ ‡å‡†åŒ–é¢„æµ‹å€¼
train_preds = y_scaler.inverse_transform(train_preds_scaled.reshape(-1, 1)).flatten()
test_preds = y_scaler.inverse_transform(test_preds_scaled.reshape(-1, 1)).flatten()

# è®¡ç®—è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ€§èƒ½æŒ‡æ ‡
train_r2, train_rmse, train_mae, train_mape = calculate_metrics(y_train, train_preds)
test_r2, test_rmse, test_mae, test_mape = calculate_metrics(y_test, test_preds)

# æ‰“å°è®­ç»ƒé›†ç»“æœ
print("\n========== Final Performance on Training Set ==========")
print(f"RÂ²: {train_r2:.4f}, RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}, MAPE: {train_mape:.4f}")

# æ‰“å°æµ‹è¯•é›†ç»“æœ
print("\n========== Final Performance on Test Set ==========")
print(f"RÂ²: {test_r2:.4f}, RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, MAPE: {test_mape:.4f}")