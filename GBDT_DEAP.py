import os
import numpy as np
import pandas as pd
from sklearn.model_selection import KFold, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingRegressor
from deap import base, creator, tools, algorithms
from metrics import calculate_metrics
import random
from sklearn.exceptions import ConvergenceWarning
import warnings

warnings.filterwarnings("ignore", category=ConvergenceWarning)
os.environ["LOKY_MAX_CPU_COUNT"] = "4"

# ==========================
# 1. è¯»å–æ•°æ®
# ==========================
file_path = r''
data = pd.read_excel(file_path, sheet_name='Sheet2')


X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values.ravel()

# ==========================
# 2. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
# ==========================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

print(f"ğŸ” è®­ç»ƒé›†å¤§å°: {X_train.shape}, æµ‹è¯•é›†å¤§å°: {X_test.shape}")

# ==========================
# 3. å®šä¹‰ K æŠ˜äº¤å‰éªŒè¯
# ==========================
n_splits = 3
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# ==========================
# 4. å®šä¹‰é—ä¼ ç®—æ³•
# ==========================
creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
creator.create("Individual", list, fitness=creator.FitnessMin)

def create_individual():
    """ç”ŸæˆGBRTè¶…å‚æ•°ä¸ªä½“"""
    n_estimators = np.random.choice([50, 100, 150, 200])
    max_depth = np.random.choice([3, 5, 10, 15])
    learning_rate = np.random.choice([0.001, 0.01, 0.05, 0.1])
    subsample = np.random.choice([0.7, 0.8, 0.9, 1.0])
    max_features = np.random.choice([0.7, 0.8, 0.9, 1.0])
    min_samples_split = np.random.choice([2, 5, 10])
    return [n_estimators, max_depth, learning_rate, subsample, max_features, min_samples_split]

def validate_params(individual):
    """å‚æ•°æœ‰æ•ˆæ€§éªŒè¯"""
    individual[0] = max(10, int(individual[0]))  # n_estimators
    individual[1] = None if individual[1] == -1 else int(individual[1])  # max_depth
    individual[2] = np.clip(individual[2], 0.001, 0.3)  # learning_rate
    individual[3] = np.clip(individual[3], 0.5, 1.0)  # subsample
    individual[4] = np.clip(individual[4], 0.5, 1.0)  # max_features
    individual[5] = max(2, int(individual[5]))  # min_samples_split
    return individual

def evaluate(individual):
    """é€‚åº”åº¦è¯„ä¼°å‡½æ•°"""
    individual = validate_params(individual.copy())
    
    (n_estimators, max_depth, learning_rate, 
     subsample, max_features, min_samples_split) = individual
    
    fold_rmse = []
    for fold, (train_idx, valid_idx) in enumerate(kf.split(X_train)):
        X_tr, X_val = X_train[train_idx], X_train[valid_idx]
        y_tr, y_val = y_train[train_idx], y_train[valid_idx]

        scaler = StandardScaler()
        X_tr_scaled = scaler.fit_transform(X_tr)
        X_val_scaled = scaler.transform(X_val)

        model = GradientBoostingRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            learning_rate=learning_rate,
            subsample=subsample,
            max_features=max_features,
            min_samples_split=min_samples_split,
            validation_fraction=0.2,
            n_iter_no_change=5,
            tol=0.01,
            random_state=42
        )
        
        model.fit(X_tr_scaled, y_tr)
        preds = model.predict(X_val_scaled)
        rmse = np.sqrt(np.mean((preds - y_val) ** 2))
        fold_rmse.append(rmse)

    return np.mean(fold_rmse),

# æ³¨å†Œå·¥å…·
toolbox = base.Toolbox()
random.seed(42)
np.random.seed(42)
toolbox.register("individual", tools.initIterate, creator.Individual, create_individual)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)
toolbox.register("mate", tools.cxTwoPoint)
toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.3)
toolbox.register("select", tools.selTournament, tournsize=3)
toolbox.register("evaluate", evaluate)

# ==========================
# 5. é—ä¼ ç®—æ³•å¾ªç¯
# ==========================
population = toolbox.population(n=30)
generations = 15
cx_prob = 0.6
mut_prob = 0.3

for gen in range(generations):
    print(f"\n===== Generation {gen + 1}/{generations} =====")
    fitnesses = list(map(toolbox.evaluate, population))
    
    for ind, fit in zip(population, fitnesses):
        ind.fitness.values = fit

    offspring = toolbox.select(population, len(population))
    offspring = list(map(toolbox.clone, offspring))

    # äº¤å‰æ“ä½œ
    for child1, child2 in zip(offspring[::2], offspring[1::2]):
        if np.random.rand() < cx_prob:
            toolbox.mate(child1, child2)
            child1[:] = validate_params(child1)
            child2[:] = validate_params(child2)
            del child1.fitness.values
            del child2.fitness.values

    # å˜å¼‚æ“ä½œ
    for mutant in offspring:
        if np.random.rand() < mut_prob:
            # æµ®ç‚¹æ•°å‚æ•°å˜å¼‚
            for i in [2, 3, 4]:
                mutant[i] += np.random.normal(0, 0.05)
            # æ•´æ•°å‚æ•°å˜å¼‚
            mutant[5] += np.random.randint(-2, 3)
            mutant[:] = validate_params(mutant)
            del mutant.fitness.values

    # é‡æ–°è¯„ä¼°æ— æ•ˆä¸ªä½“
    invalid_individuals = [ind for ind in offspring if not ind.fitness.valid]
    fitnesses = list(map(toolbox.evaluate, invalid_individuals))
    for ind, fit in zip(invalid_individuals, fitnesses):
        ind.fitness.values = fit

    population[:] = offspring

# ==========================
# 6. é€‰å–æœ€ä¼˜è¶…å‚æ•°
# ==========================
best_individual = tools.selBest(population, 1)[0]
best_individual = validate_params(best_individual)
best_params = {
    'n_estimators': int(best_individual[0]),
    'max_depth': best_individual[1],
    'learning_rate': float(best_individual[2]),
    'subsample': float(best_individual[3]),
    'max_features': float(best_individual[4]),
    'min_samples_split': int(best_individual[5])
}

print(f"\nğŸ¯ é€‰å–æœ€ä¼˜è¶…å‚æ•°: {best_params}")

# ==========================
# 7. è®­ç»ƒæœ€ç»ˆæ¨¡å‹å¹¶è¯„ä¼°
# ==========================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

final_gbrt = GradientBoostingRegressor(**best_params, random_state=42)
final_gbrt.fit(X_train_scaled, y_train)

# è®¡ç®—è®­ç»ƒé›†è¯¯å·®
gbrt_train_preds = final_gbrt.predict(X_train_scaled)
gbrt_train_r2, gbrt_train_rmse, gbrt_train_mae, gbrt_train_mape = calculate_metrics(y_train, gbrt_train_preds)

# è®¡ç®—æµ‹è¯•é›†è¯¯å·®
gbrt_test_preds = final_gbrt.predict(X_test_scaled)
gbrt_test_r2, gbrt_test_rmse, gbrt_test_mae, gbrt_test_mape = calculate_metrics(y_test, gbrt_test_preds)

# æ‰“å°ç»“æœ
print("\n========== Final Performance on Training Set ==========")
print(f"ğŸ“Œ GBRT - RÂ²: {gbrt_train_r2:.4f}, RMSE: {gbrt_train_rmse:.4f}, MAE: {gbrt_train_mae:.4f}, MAPE: {gbrt_train_mape:.4f}")

print("\n========== Final Performance on Test Set ==========")
print(f"ğŸ“Œ GBRT - RÂ²: {gbrt_test_r2:.4f}, RMSE: {gbrt_test_rmse:.4f}, MAE: {gbrt_test_mae:.4f}, MAPE: {gbrt_test_mape:.4f}")

# å®šä¹‰è¦ç»˜åˆ¶PDPçš„ç‰¹å¾ç´¢å¼•ã€‚
from sklearn.inspection import partial_dependence
import matplotlib.pyplot as plt
import numpy as np
plt.rcParams['font.family'] = ["Times New Roman", "SimSun"]
# features = [1, 4] è¡¨ç¤ºç¬¬äºŒä¸ªç‰¹å¾ï¼ˆç´¢å¼•ä¸º1ï¼‰å’Œç¬¬äº”ä¸ªç‰¹å¾ï¼ˆç´¢å¼•ä¸º4ï¼‰ã€‚
# è¯·æ³¨æ„ï¼ŒPythonçš„ç´¢å¼•æ˜¯ä»0å¼€å§‹çš„ã€‚
features = [8, 3]
# è®¡ç®—åä¾èµ–æ€§
# grid_resolution=50 è¡¨ç¤ºæ¯ä¸ªç‰¹å¾çš„ç½‘æ ¼ç‚¹æ•°é‡ä¸º50ï¼Œè¿™å°†ç”Ÿæˆä¸€ä¸ª50x50çš„ç½‘æ ¼
# kind='average' æ˜ç¡®è¡¨ç¤ºæˆ‘ä»¬æƒ³è¦å¹³å‡çš„é¢„æµ‹ç»“æœ
pdp_result = partial_dependence(
    final_gbrt,
    X_test_scaled,
    features=features,
    grid_resolution=50,
    kind='average'
)

# ä»pdp_resultä¸­æå–ç½‘æ ¼å€¼å’Œå¹³å‡é¢„æµ‹ç»“æœ
# pdp_result['grid_values'][0] å¯¹åº”äº features åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªç‰¹å¾ (ç´¢å¼•1)
# pdp_result['grid_values'][1] å¯¹åº”äº features åˆ—è¡¨ä¸­çš„ç¬¬äºŒä¸ªç‰¹å¾ (ç´¢å¼•4)
XX, YY = np.meshgrid(pdp_result['grid_values'][0], pdp_result['grid_values'][1])

# pdp_result['average'] å¯¹äºäºŒç»´PDPä¼šè¿”å›ä¸€ä¸ªåŒ…å«å•ä¸ªæ•°ç»„çš„åˆ—è¡¨ã€‚
# è¿™ä¸ªæ•°ç»„å°±æ˜¯æˆ‘ä»¬éœ€è¦ç»˜åˆ¶çš„äºŒç»´åä¾èµ–å€¼ã€‚
Z = pdp_result['average'][0]

# åˆ›å»ºå›¾å½¢å’Œå­å›¾
plt.figure(figsize=(9, 7)) # è®¾ç½®å›¾çš„å¤§å°ï¼Œç•¥å¾®å¢å¤§ä»¥æé«˜å¯è¯»æ€§

# ç»˜åˆ¶ç­‰é«˜çº¿å¡«å……å›¾
# cmap='viridis' è®¾ç½®é¢œè‰²æ˜ å°„
# levels=20 å¢åŠ äº†ç­‰é«˜çº¿çš„æ•°é‡ï¼Œä½¿é¢œè‰²è¿‡æ¸¡æ›´å¹³æ»‘
cp = plt.contourf(XX, YY, Z, cmap='viridis', levels=20)

# æ·»åŠ é¢œè‰²æ¡ï¼Œå¹¶è®¾ç½®å…¶æ ‡ç­¾
plt.colorbar(cp, label='å¹³å‡é¢„æµ‹ç›®æ ‡å€¼')

# è®¾ç½®Xè½´å’ŒYè½´çš„æ ‡ç­¾ã€‚
# æ ‡ç­¾ä¼šæ ¹æ®æ‚¨åœ¨ features ä¸­å®šä¹‰çš„ç‰¹å¾ç´¢å¼•åŠ¨æ€ç”Ÿæˆã€‚
plt.xlabel(f'ç‰¹å¾ {features[0]+1} (ç´¢å¼• {features[0]})')
plt.ylabel(f'ç‰¹å¾ {features[1]+1} (ç´¢å¼• {features[1]})')
plt.title(f'äºŒç»´åä¾èµ–å›¾ï¼ˆç‰¹å¾ {features[0]+1} å’Œ {features[1]+1}ï¼‰')
plt.show()


