import numpy as np
import pandas as pd
from sklearn.model_selection import KFold, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, RegressorMixin
from deap import base, creator, tools, algorithms
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
import random
import tensorflow as tf

# è®¾ç½®æ—¥å¿—çº§åˆ«ä¸º ERRORï¼Œå‡å°‘æ—¥å¿—è¾“å‡º
tf.get_logger().setLevel('ERROR')

# ==========================
# 1. è¯»å–æ•°æ®
# ==========================
file_path = r''
data = pd.read_excel(file_path, sheet_name='Sheet2')

X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values.reshape(-1, 1)

# ==========================
# 2. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
# ==========================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

print(f"ğŸ” è®­ç»ƒé›†å¤§å°: {X_train.shape}, æµ‹è¯•é›†å¤§å°: {X_test.shape}")

y_scaler = StandardScaler()
y_train_scaled = y_scaler.fit_transform(y_train)
y_test_scaled = y_scaler.transform(y_test)

# ==========================
# 3. å®šä¹‰ K æŠ˜äº¤å‰éªŒè¯
# ==========================
n_splits = 3
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# ==========================
# 4. å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹
# ==========================
def create_nn_model(hidden_layer_size=64):
    model = Sequential()
    model.add(Dense(units=hidden_layer_size, activation='relu', input_dim=X_train.shape[1]))
    model.add(Dense(units=32, activation='relu'))
    model.add(Dense(units=1))  # è¾“å‡ºå›å½’å€¼
    model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mse'])
    return model

# ==========================
# 5. è‡ªå®šä¹‰ Keras æ¨¡å‹åŒ…è£…å™¨
# ==========================
class KerasRegressorWrapper(BaseEstimator, RegressorMixin):
    def __init__(self, build_fn, hidden_layer_size=64, epochs=100, batch_size=32, verbose=0):
        self.build_fn = build_fn
        self.hidden_layer_size = hidden_layer_size
        self.epochs = epochs
        self.batch_size = batch_size
        self.verbose = verbose
        self.model = None

    def fit(self, X, y):
        self.model = self.build_fn(hidden_layer_size=self.hidden_layer_size)
        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)
        return self

    def predict(self, X):
        return self.model.predict(X, verbose=0).flatten()

    def score(self, X, y):
        return self.model.evaluate(X, y, verbose=0)[1]

# ==========================
# 6. å®šä¹‰è¯„ä¼°æŒ‡æ ‡å‡½æ•°
# ==========================
def calculate_metrics(y_true, y_pred):
    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
    r2 = r2_score(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    return r2, rmse, mae, mape

# ==========================
# 7. å®šä¹‰é—ä¼ ç®—æ³•éƒ¨åˆ†
# ==========================
creator.create("FitnessMin", base.Fitness, weights=(-1.0,))  # æœ€å°åŒ–ç›®æ ‡
creator.create("Individual", list, fitness=creator.FitnessMin)

def create_individual():
    """ç”Ÿæˆä¸€ä¸ªä¸ªä½“ï¼ˆè¶…å‚æ•°ç»„åˆï¼‰"""
    hidden_layer_size = random.choice([16, 32, 64])  # æ‰©å¤§æœç´¢èŒƒå›´
    batch_size = random.choice([16, 32, 64])  # æ‰©å¤§æœç´¢èŒƒå›´
    epochs = random.choice([50, 100, 150])  # æ‰©å¤§æœç´¢èŒƒå›´
    return [hidden_layer_size, batch_size, epochs]

def validate_params(individual):
    """ç¡®ä¿å‚æ•°åœ¨æœ‰æ•ˆèŒƒå›´å†…"""
    individual[0] = max(32, int(individual[0]))  # hidden_layer_size
    individual[1] = max(16, int(individual[1]))  # batch_size
    individual[2] = max(1, int(individual[2]))   # epochs
    return individual

def evaluate(individual):
    """è¯„ä¼°ä¸ªä½“ï¼ˆè¶…å‚æ•°ç»„åˆï¼‰çš„è¡¨ç°"""
    individual = validate_params(individual.copy())  # ç¡®ä¿å‚æ•°åˆæ³•
    
    hidden_layer_size, batch_size, epochs = individual
    
    fold_rmse = []
    for fold, (train_idx, valid_idx) in enumerate(kf.split(X_train)):
        X_train_fold, X_valid_fold = X_train[train_idx], X_train[valid_idx]
        y_train_fold, y_valid_fold = y_train[train_idx], y_train[valid_idx]

        scaler = StandardScaler()
        X_train_fold_scaled = scaler.fit_transform(X_train_fold)
        X_valid_fold_scaled = scaler.transform(X_valid_fold)

        # å®šä¹‰æ¨¡å‹
        model = create_nn_model(hidden_layer_size=hidden_layer_size)
        model.fit(X_train_fold_scaled, y_train_fold, epochs=epochs, batch_size=batch_size, verbose=0)  # ç¦ç”¨è¾“å‡º

        # é¢„æµ‹
        preds = model.predict(X_valid_fold_scaled).flatten()
        _, rmse, _, _ = calculate_metrics(y_valid_fold, preds)

        fold_rmse.append(rmse)

    return np.mean(fold_rmse),  # è¿”å›ä¸€ä¸ªå…ƒç»„ï¼Œé—ä¼ ç®—æ³•éœ€è¦

toolbox = base.Toolbox()
toolbox.register("individual", tools.initIterate, creator.Individual, create_individual)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)
toolbox.register("mate", tools.cxTwoPoint)  # ä½¿ç”¨ä¸¤ç‚¹äº¤å‰
toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=0.05, indpb=0.2)  # é«˜æ–¯å˜å¼‚
toolbox.register("select", tools.selTournament, tournsize=3)
toolbox.register("evaluate", evaluate)

# ==========================
# 8. é—ä¼ ç®—æ³•å¾ªç¯
# ==========================
population = toolbox.population(n=30)  # å¢åŠ ç§ç¾¤å¤§å°
generations = 15  # å¢åŠ ä»£æ•°
cx_prob = 0.7
mut_prob = 0.2

# è®¾ç½®éšæœºç§å­
random.seed(42)
np.random.seed(42)

for gen in range(generations):
    print(f"\n===== Generation {gen + 1}/{generations} =====")
    fitnesses = list(map(toolbox.evaluate, population))
    
    for ind, fit in zip(population, fitnesses):
        ind.fitness.values = fit

    offspring = toolbox.select(population, len(population))
    offspring = list(map(toolbox.clone, offspring))

    # äº¤å‰æ“ä½œ
    for child1, child2 in zip(offspring[::2], offspring[1::2]):
        if np.random.rand() < cx_prob:
            toolbox.mate(child1, child2)
            child1[:] = validate_params(child1)
            child2[:] = validate_params(child2)
            del child1.fitness.values
            del child2.fitness.values

    # å˜å¼‚æ“ä½œ
    for mutant in offspring:
        if np.random.rand() < mut_prob:
            # ä»…å¯¹æµ®ç‚¹æ•°å‚æ•°è¿›è¡Œå˜å¼‚
            for i in [0, 1, 2]:  # hidden_layer_size, batch_size, epochs
                mutant[i] += np.random.normal(0, 0.05)  # æ›´å°çš„å˜å¼‚æ­¥é•¿
            mutant[:] = validate_params(mutant)
            del mutant.fitness.values

    # é‡æ–°è¯„ä¼°æ— æ•ˆä¸ªä½“
    invalid_individuals = [ind for ind in offspring if not ind.fitness.valid]
    fitnesses = list(map(toolbox.evaluate, invalid_individuals))
    for ind, fit in zip(invalid_individuals, fitnesses):
        ind.fitness.values = fit

    population[:] = offspring

# ==========================
# 9. é€‰å–æœ€ä¼˜è¶…å‚æ•°
# ==========================
best_individual = tools.selBest(population, 1)[0]
best_individual = validate_params(best_individual)  # æœ€ç»ˆéªŒè¯å‚æ•°
best_params = {
    'hidden_layer_size': int(best_individual[0]),
    'batch_size': int(best_individual[1]),
    'epochs': int(best_individual[2])
}

print(f"\nğŸ¯ é€‰å–æœ€ä¼˜è¶…å‚æ•°: {best_params}")

# ==========================
# 10. åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹
# ==========================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ä½¿ç”¨æœ€ä½³è¶…å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
final_model = create_nn_model(hidden_layer_size=best_params['hidden_layer_size'])
final_model.fit(X_train_scaled, y_train_scaled, epochs=best_params['epochs'], batch_size=best_params['batch_size'], verbose=0)

# å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
train_preds_scaled = final_model.predict(X_train_scaled)
test_preds_scaled = final_model.predict(X_test_scaled)

# åæ ‡å‡†åŒ–é¢„æµ‹å€¼
train_preds = y_scaler.inverse_transform(train_preds_scaled.reshape(-1, 1)).flatten()
test_preds = y_scaler.inverse_transform(test_preds_scaled.reshape(-1, 1)).flatten()

# è®¡ç®—è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ€§èƒ½æŒ‡æ ‡
train_r2, train_rmse, train_mae, train_mape = calculate_metrics(y_train, train_preds)
test_r2, test_rmse, test_mae, test_mape = calculate_metrics(y_test, test_preds)

# ==========================
# 9. è¾“å‡ºè®­ç»ƒé›†ä¸æµ‹è¯•é›†çš„çœŸå®å€¼å’Œé¢„æµ‹å€¼ï¼Œå¹¶ä¿å­˜
# ==========================
# æ‰“å°è®­ç»ƒé›†ç»“æœ
print("\n========== Final Performance on Training Set ==========")
print(f"ğŸ“Œ FNN - RÂ² (Train): {train_r2:.4f}, RMSE (Train): {train_rmse:.4f}, MAE (Train): {train_mae:.4f}, MAPE (Train): {train_mape:.4f}")

# æ‰“å°æµ‹è¯•é›†ç»“æœ
print("\n========== Final Performance on Test Set ==========")
print(f"ğŸ“Œ FNN - RÂ² (Test): {test_r2:.4f}, RMSE (Test): {test_rmse:.4f}, MAE (Test): {test_mae:.4f}, MAPE (Test): {test_mape:.4f}")

# ==========================
# ç»“æœå‡†å¤‡ï¼šå°†è®­ç»ƒé›†ä¸æµ‹è¯•é›†çš„ç»“æœåˆ†åˆ«å­˜å‚¨
# ==========================
# åˆ›å»ºè®­ç»ƒé›†çš„ç»“æœè¡¨æ ¼
train_results_df = pd.DataFrame({
    "True Values (Train)": y_train.flatten(),
    "FNN Train Predictions": train_preds.flatten(),
})

# åˆ›å»ºæµ‹è¯•é›†çš„ç»“æœè¡¨æ ¼
test_results_df = pd.DataFrame({
    "True Values (Test)": y_test.flatten(),
    "FNN Test Predictions": test_preds.flatten(),
})

# åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç»“æœ
results_df = pd.concat([train_results_df, test_results_df], axis=1)

# ==========================
# ç»“æœä¿å­˜ï¼šå°†æœ€ä¼˜è¶…å‚æ•°ä½œä¸ºæè¿°ä¿¡æ¯æ·»åŠ 
# ==========================
# å®šä¹‰æœ€ä¼˜è¶…å‚æ•°çš„å­—ç¬¦ä¸²å½¢å¼
best_params_str = str(best_params)  # å°†å­—å…¸è½¬æ¢ä¸ºå­—ç¬¦ä¸²

# å°†æœ€ä¼˜è¶…å‚æ•°å­˜å…¥ç¬¬ä¸€è¡Œ
results_df["Best Params"] = None
results_df.at[0, "Best Params"] = best_params_str

# ä¿å­˜ç»“æœåˆ° CSV æ–‡ä»¶
results_df.to_csv("FNN_results.csv", index=False)
print("\nâœ… é¢„æµ‹ç»“æœå’Œæœ€ä¼˜è¶…å‚æ•°å·²ä¿å­˜åˆ°æ–‡ä»¶ï¼šFNN_results.csv")
