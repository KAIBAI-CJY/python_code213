import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from metrics import calculate_metrics  # ç¡®ä¿ metrics.py ä¸­æœ‰ calculate_metrics å‡½æ•°
import matplotlib.ticker as ticker

# è®¾ç½®æ–‡ä»¶å¤¹è·¯å¾„
base_folder = ""  # æœªä¼˜åŒ–çš„åŸºç¡€æ¨¡å‹
deap_folder = ""    # é—ä¼ ç®—æ³•ä¼˜åŒ–çš„æ¨¡å‹

# è®¾ç½®å…¨å±€å­—ä½“ï¼ˆä¸­æ–‡ä¸ºå®‹ä½“ï¼Œè‹±æ–‡ä¸º Times New Romanï¼‰
plt.rcParams['font.family'] = ["Times New Roman", "SimSun"]

# æ¨¡å‹æ–‡ä»¶åï¼ˆæœªä¼˜åŒ– vs. ä¼˜åŒ–åï¼‰
base_files = [
    "01_MLP_results_base3.csv",
    "02_RBF_results_base3.csv",
    "03_RF_results_base3.csv",
    "04_Ridge_results_base3.csv",
    "05_XGBoost_results_base3.csv",
    "06_TabPFN_results_base3.csv"  # æ–°æ·»åŠ çš„æ¨¡å‹
]
deap_files = [
    "01_MLP_results3.csv",
    "02_RBF_results3.csv",
    "03_RF_results3.csv",
    "04_Ridge_results3.csv",
    "05_XGBoost_results3.csv"
]
model_names = ["MLP", "RBF", "RF", "Ridge", "XGBoost", "TabPFN"]  # æ–°æ¨¡å‹æ·»åŠ åˆ°æ¨¡å‹åç§°åˆ—è¡¨

# è¯¯å·®å­˜å‚¨
metrics_dict = {
    "Model": [],
    "Optimization": [],
    "Set": [],
    "RÂ²": [],
    "RMSE": [],
    "MAE": [],
    "MAPE": []
}

# éå†ä¸¤ç»„æ–‡ä»¶ï¼Œè®¡ç®—è®­ç»ƒ & æµ‹è¯•é›†è¯¯å·®
for base_file, deap_file, model_name in zip(base_files, deap_files + [None],
                                            model_names):  # deap_files + [None] æ¥å¤„ç†æ²¡æœ‰ä¼˜åŒ–çš„TabPFN
    for folder, opt_label in zip([base_folder, deap_folder], ["Default", "Tuned(GA)"]):
        if model_name == "TabPFN" and opt_label == "Tuned(GA)":  # TabPFNæ²¡æœ‰ä¼˜åŒ–æ–‡ä»¶
            continue

        file_path = os.path.join(folder, deap_file if opt_label == "Tuned(GA)" else base_file)

        # ç¡®ä¿æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(file_path):
            print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
            continue

        # è¯»å– CSVï¼Œè·³è¿‡ç¬¬ä¸€è¡Œ
        df = pd.read_csv(file_path, skiprows=0)

        # è®­ç»ƒé›†æ•°æ®
        df_train = df.iloc[:, :2].dropna()
        y_train_true = df_train.iloc[:, 0].values
        y_train_pred = df_train.iloc[:, 1].values

        # æµ‹è¯•é›†æ•°æ®
        df_test = df.iloc[:, 2:4].dropna()
        y_test_true = df_test.iloc[:, 0].values
        y_test_pred = df_test.iloc[:, 1].values

        print(f"ğŸ” è®­ç»ƒé›†å¤§å°: {y_train_true.shape}, è®­ç»ƒé›†å¤§å°: {y_train_pred.shape}")
        print(f"ğŸ” æµ‹è¯•é›†å¤§å°: {y_test_true.shape}, æµ‹è¯•é›†å¤§å°: {y_test_pred.shape}")

        # è®¡ç®—è¯¯å·®
        train_r2, train_rmse, train_mae, train_mape = calculate_metrics(y_train_true, y_train_pred)
        test_r2, test_rmse, test_mae, test_mape = calculate_metrics(y_test_true, y_test_pred)

        # å­˜å‚¨è®­ç»ƒé›†è¯¯å·®
        metrics_dict["Model"].append(model_name)
        metrics_dict["Optimization"].append(opt_label)
        metrics_dict["Set"].append("Train")
        metrics_dict["RÂ²"].append(train_r2)
        metrics_dict["RMSE"].append(train_rmse)
        metrics_dict["MAE"].append(train_mae)
        metrics_dict["MAPE"].append(train_mape)

        # å­˜å‚¨æµ‹è¯•é›†è¯¯å·®
        metrics_dict["Model"].append(model_name)
        metrics_dict["Optimization"].append(opt_label)
        metrics_dict["Set"].append("Test")
        metrics_dict["RÂ²"].append(test_r2)
        metrics_dict["RMSE"].append(test_rmse)
        metrics_dict["MAE"].append(test_mae)
        metrics_dict["MAPE"].append(test_mape)

# è½¬æ¢ä¸º DataFrame
metrics_df = pd.DataFrame(metrics_dict)

# ç”» 4 ä¸ªå­å›¾ï¼ˆRÂ²ã€RMSEã€MAEã€MAPEï¼‰
fig, axes = plt.subplots(2, 2, figsize=(16, 12))  # è°ƒæ•´å­å›¾æ’åˆ—ä¸º2è¡Œ2åˆ—
metrics_names = ["RÂ²", "RMSE", "MAE", "MAPE"]
colors = {
    'Train_Default': '#5b9bd5',  # æ·±è“è‰²ï¼Œè®­ç»ƒé›† - æœªä¼˜åŒ–
    'Test_Default': '#FFB5A3',  # æµ…çº¢è‰²ï¼Œæµ‹è¯•é›† - æœªä¼˜åŒ–
    'Train_Tuned': '#2ca02c',  # ç»¿è‰²ï¼Œè®­ç»ƒé›† - ä¼˜åŒ–å
    'Test_Tuned': '#d62728'  # çº¢è‰²ï¼Œæµ‹è¯•é›† - ä¼˜åŒ–å
}

for i, metric in enumerate(metrics_names):
    ax = axes[i // 2, i % 2]  # è®¡ç®—å½“å‰å­å›¾çš„è¡Œåˆ—ä½ç½®

    # è®­ç»ƒé›† & æµ‹è¯•é›† æ•°æ®åˆ†å¼€
    for dataset, bar_offset in zip(["Train", "Test"], [-0.2, 0.2]):
        base_values = []
        tuned_values = []

        for model in model_names:
            base_val = metrics_df[
                (metrics_df["Model"] == model) &
                (metrics_df["Optimization"] == "Default") &
                (metrics_df["Set"] == dataset)
                ][metric].values[0]

            if model == "TabPFN":  # TabPFN ä¸å­˜åœ¨ä¼˜åŒ–ç‰ˆæœ¬
                tuned_val = np.nan  # ä¸ºäº†é¿å…æŠ¥é”™ï¼Œä¼˜åŒ–å€¼è®¾ç½®ä¸º NaN
            else:
                tuned_val = metrics_df[
                    (metrics_df["Model"] == model) &
                    (metrics_df["Optimization"] == "Tuned(GA)") &
                    (metrics_df["Set"] == dataset)
                    ][metric].values[0]

            base_values.append(base_val)
            tuned_values.append(tuned_val)

        if metric == "RÂ²":  # RÂ²ï¼šä¼˜åŒ–åçš„ä¸ºå¤§æŸ±å­ï¼Œæœªä¼˜åŒ–ä¸ºåµŒå¥—å°æŸ±å­
            ax.bar(
                np.arange(len(model_names)) + bar_offset,
                tuned_values,
                width=0.4,
                label=f"{dataset} - Tuned(GA)",
                color=colors[f'{dataset}_Tuned'],
                alpha=0.9  # è®¾ç½®é€æ˜åº¦ï¼Œå¢å¼ºå¯¹æ¯”åº¦
            )
            ax.bar(
                np.arange(len(model_names)) + bar_offset,
                base_values,
                width=0.2,
                label=f"{dataset} - Default",
                color=colors[f'{dataset}_Default'],
                alpha=0.6  # è®¾ç½®é€æ˜åº¦ï¼Œå¢å¼ºå¯¹æ¯”åº¦
            )

            # è°ƒæ•´Yè½´èŒƒå›´å¹¶æ‰“æ–­0.1åˆ°0.5ä¹‹é—´çš„éƒ¨åˆ†ï¼Œå¢å¼ºå¯¹æ¯”
            ax.tick_params(axis='y', labelsize=14)  # è®¾ç½® y è½´åˆ»åº¦å­—ä½“å¤§å°ä¸º 14
            ax.set_ylim(0, 1.1)
            ax.set_yticks(np.linspace(0, 1.1, 7))  # è®¾ç½®yè½´åˆ»åº¦
            ax.set_yticklabels(np.round(np.linspace(0, 1.1, 7), 2))  # è®¾ç½®yè½´åˆ»åº¦æ ‡ç­¾

            # æ‰“æ–­yè½´çš„çº¿æ¡
            ax.spines['top'].set_visible(True)  # æ·»åŠ ä¸Šè¾¹æ¡†
            ax.spines['top'].set_color('black')  # è®¾ç½®è¾¹æ¡†é¢œè‰²ä¸ºé»‘è‰²
            ax.spines['top'].set_linewidth(1.0)  # è®¾ç½®è¾¹æ¡†çº¿å®½
            ax.spines['bottom'].set_color('none')

        else:  # RMSEã€MAEã€MAPEï¼šä¼˜åŒ–åçš„ä¸ºåµŒå¥—å°æŸ±å­ï¼Œæœªä¼˜åŒ–ä¸ºå¤§æŸ±å­
            ax.bar(
                np.arange(len(model_names)) + bar_offset,
                base_values,
                width=0.4,
                label=f"{dataset} - Default",
                color=colors[f'{dataset}_Default'],
                alpha=0.9  # è®¾ç½®é€æ˜åº¦ï¼Œå¢å¼ºå¯¹æ¯”åº¦
            )
            ax.bar(
                np.arange(len(model_names)) + bar_offset,
                tuned_values,
                width=0.2,
                label=f"{dataset} - Tuned(GA)",
                color=colors[f'{dataset}_Tuned'],
                alpha=0.6  # è®¾ç½®é€æ˜åº¦ï¼Œå¢å¼ºå¯¹æ¯”åº¦
            )
    ax.tick_params(axis='y', labelsize=14)  # è®¾ç½® y è½´åˆ»åº¦å­—ä½“å¤§å°ä¸º 14
    ax.set_ylabel(metric, fontsize=14)
    ax.set_xticks(np.arange(len(model_names)))
    ax.set_xticklabels(model_names, rotation=30, fontsize=14)

    # è®¾ç½®å›¾ä¾‹ä½ç½®
    if metric == "RÂ²":
        ax.legend(loc='lower left', fontsize=14)  # RÂ²å›¾ä¾‹æ”¾åœ¨å·¦ä¸‹è§’
    else:
        ax.legend(loc='upper right', fontsize=14)  # å…¶ä»–å›¾çš„å›¾ä¾‹æ”¾åœ¨å³ä¸Šè§’

plt.tight_layout()
plt.show()


# ä¿å­˜è¯¯å·®æ•°æ®
output_path = os.path.join(deap_folder, "Model_Performance_Comparison3.csv")
metrics_df.to_csv(output_path, index=False)
print(f"âœ… è¯¯å·®æ•°æ®å·²ä¿å­˜è‡³ {output_path}")


